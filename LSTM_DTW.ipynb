{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Jus35OVNDbUS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load the packages\n",
    "#import numpy\n",
    "import pandas\n",
    "#import io\n",
    "import os\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "#following required for LSTM\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "#following requried for DTW_kNN\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from tslearn.generators import random_walk_blobs\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax, TimeSeriesScalerMeanVariance\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier, KNeighborsTimeSeries\n",
    "#set the working directory\n",
    "os.chdir(\"/group/yleung/data/genotype_prediction/seperate_on_off/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2array(df, YellowPage, fold):\n",
    "    test_uid = YellowPage.loc[YellowPage.iloc[:,fold+1] == 'Test']['uid']\n",
    "    train_uid = YellowPage.loc[YellowPage.iloc[:,fold+1] == 'Train']['uid']\n",
    "    #subset the dataset\n",
    "    test_sub = df.loc[df['uid'].isin(test_uid)]\n",
    "    train_sub = df.loc[df['uid'].isin(train_uid)]\n",
    "    #transform from long format to 3-d array, [sample, time_step, variable]\n",
    "    test_list = [x for _, x in test_sub.groupby(['uid'])]\n",
    "    test_values = np.array([d_f.values for d_f in test_list])\n",
    "    train_list = [x for _, x in train_sub.groupby(['uid'])]\n",
    "    train_values = np.array([d_f.values for d_f in train_list])\n",
    "    #make sure all data is float\n",
    "    train = np.asarray(train_values[:,:,:(train_values.shape[2]-1)]).astype(np.float32)\n",
    "    test = np.asarray(test_values[:,:,:(test_values.shape[2]-1)]).astype(np.float32)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(x, y):\n",
    "    #design the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape= x[0].shape[1:])) #training X shape\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "    #fit the model\n",
    "    history = model.fit(x[0], y[0], epochs = 100, batch_size = 64, verbose = 0) #training X and training Y\n",
    "    \n",
    "    #predict the test\n",
    "    test_pred = model.predict(x[1])\n",
    "    #confusion matrix\n",
    "    test_matrix = metrics.confusion_matrix(y[1], np.rint(test_pred))\n",
    "    \n",
    "    #break down\n",
    "    tn = test_matrix[0, 0]\n",
    "    tp = test_matrix[1, 1]\n",
    "    fn = test_matrix[1, 0]\n",
    "    fp = test_matrix[0, 1]\n",
    "    #output metrics\n",
    "    acc = metrics.accuracy_score(y[1], np.rint(test_pred)) #accuracy\n",
    "    sns = metrics.recall_score(y[1], np.rint(test_pred)) #sensitivity = recall\n",
    "    sps = tn / (tn + fp) #specificity\n",
    "    prc = metrics.precision_score(y[1], np.rint(test_pred)) #precision\n",
    "    kappa = metrics.cohen_kappa_score(y[1], np.rint(test_pred)) #Cohen's kappa\n",
    "    auroc = metrics.roc_auc_score(y[1], np.rint(test_pred)) #area under ROC curve\n",
    "\n",
    "    perform_metrics = [acc, sns, sps, prc, kappa, auroc]\n",
    "    \n",
    "    return  perform_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTW_knn(x, y): \n",
    "    #design the model\n",
    "    knn_clf = KNeighborsTimeSeriesClassifier(n_neighbors=3, metric=\"dtw\")\n",
    "    knn_clf.fit(x[0], y[0])\n",
    "    #predict the outcome\n",
    "    predicted_labels = knn_clf.predict(x[1])\n",
    "    #confusion matrix\n",
    "    test_matrix = metrics.confusion_matrix(y[1], predicted_labels)\n",
    "    \n",
    "    #break down\n",
    "    tn = test_matrix[0, 0]\n",
    "    tp = test_matrix[1, 1]\n",
    "    fn = test_matrix[1, 0]\n",
    "    fp = test_matrix[0, 1]\n",
    "    #output metrics\n",
    "    acc = metrics.accuracy_score(y[1], predicted_labels) #accuracy\n",
    "    sns = metrics.recall_score(y[1], predicted_labels) #sensitivity = recall\n",
    "    sps = tn / (tn + fp) #specificity\n",
    "    prc = metrics.precision_score(y[1], predicted_labels) #precision\n",
    "    kappa = metrics.cohen_kappa_score(y[1], predicted_labels) #Cohen's kappa\n",
    "    auroc = metrics.roc_auc_score(y[1], predicted_labels) #area under ROC curve\n",
    "\n",
    "    perform_metrics = [acc, sns, sps, prc, kappa, auroc]\n",
    "    \n",
    "    return  perform_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a function to load, preprocess and build the LSTM model\n",
    "def CV(Data, Yellow_Page):\n",
    "    #create a list to store the accuracy for each fold\n",
    "    LSTM_fold_perm = []\n",
    "    DTWkNN_fold_perm = []\n",
    "    for k in range(1,11):\n",
    "        #split the train \n",
    "        train, test = df2array(Data, Yellow_Page, k)\n",
    "        #split the indenpendent and response variables\n",
    "        X = (train[:, :, 2:], test[:, :, 2:])# X of train and test\n",
    "        Y = (train[:, :, 0][:,0].astype(np.intc), test[:, :, 0][:,0].astype(np.intc))#Y of train and test\n",
    "        #apply each model for the independent and response variables\n",
    "        LSTM_fold_perm.append(LSTM_model(X, Y))\n",
    "        DTWkNN_fold_perm.append(DTW_knn(X, Y))\n",
    "        #print an indicator\n",
    "        print(\"Fold Complete:\", k)\n",
    "    #calculate the mean and sd across folds\n",
    "    cv_mean_sd = np.array([np.mean(LSTM_fold_perm, axis = 0),\n",
    "                          np.std(LSTM_fold_perm, axis = 0),\n",
    "                          np.mean(DTWkNN_fold_perm, axis = 0), \n",
    "                          np.std(DTWkNN_fold_perm, axis = 0)]) \n",
    "     \n",
    "    ##convert the numpy array to pandas data frame\n",
    "    #row names and column names\n",
    "    row_names = [\"LSTM\" + x for x in [\".mean\", \".sd\"]] + [\"DTW\" + x for x in [\".mean\", \".sd\"]]\n",
    "    col_names = [\"Accuracy\", \"Sensitivity\", \"Specificity\", \"Precision\", \"Kappa\", \"AUROC\"]\n",
    "    #convert the numpy array to dataframe\n",
    "    output = pandas.DataFrame(cv_mean_sd, columns=col_names, index = row_names)\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function\n",
    "#FUNCTION: preprocess the data\n",
    "def main(fname, sec):\n",
    "    #get the filename without dir and file extension\n",
    "    prefix = 'py_input_data/'\n",
    "    suffix = '.csv'\n",
    "    f_core = fname[len(prefix):][:-len(suffix)] #parse the core identifier for each dataset\n",
    "    #read the csv file to a data frame\n",
    "    data_frame = pandas.read_csv(fname, sep = \",\", decimal='.')\n",
    "    #remove the an and location column\n",
    "    dataset = data_frame.drop(['an', 'location', 'end', 'batch'], axis=1)\n",
    "    #specify the cutting time point based on different time point\n",
    "    #parse the time segment\n",
    "    dataset_sub = dataset[dataset['start'] < sec]\n",
    "    #encode the genotype label\n",
    "    encoder = LabelEncoder()\n",
    "    dataset_sub['genotype'] = encoder.fit_transform(dataset_sub['genotype']) # 0 = mut, 1 = WT\n",
    "    \n",
    "    #get the corresponding yellow page filename\n",
    "    yellow_page_prefix = 'int_output/'\n",
    "    yellow_page_suffix = '_kFolds_yellow_page.csv'\n",
    "    yellow_page_file = yellow_page_prefix + f_core + yellow_page_suffix\n",
    "    #read the yellow page\n",
    "    yellow_page = pandas.read_csv(yellow_page_file, sep = \",\", decimal='.')\n",
    "   \n",
    "    #apply the cross validation function\n",
    "    Acc_Df = CV(dataset_sub, yellow_page)\n",
    "    #set the output directory and filenames\n",
    "    out_path = \"output/\" + f_core + \"/\"\n",
    "    f_pre = f_core + \"_\" + str(sec)\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)#create the output dir for each dataset\n",
    "    #save the accuracy dataframe to an output csv file\n",
    "    Acc_Df.to_csv(out_path + f_pre + '_py_10CV.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42171/876398946.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_sub['genotype'] = encoder.fit_transform(dataset_sub['genotype']) # 0 = mut, 1 = WT\n",
      "2023-05-23 17:51:53.015674: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-23 17:51:53.026751: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 24. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Complete: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "input_files = glob.glob(\"py_input_data/*.csv\")\n",
    "time_sec = [5, 30, 60, 120, 300]\n",
    "for f in input_files:\n",
    "    for t in time_sec:\n",
    "        main(f, t)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
